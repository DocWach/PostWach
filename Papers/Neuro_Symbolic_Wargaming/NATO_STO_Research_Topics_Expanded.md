# NATO STO Research Topics: Expanded Research Questions

**Document Version:** 1.0
**Date:** 2026-01-22
**Author:** University of Arizona PostDoc Research
**Status:** TAP-Ready Draft
**Base Document:** NATO STO Potential Topics.pdf

---

## Overview

This document presents an expanded set of research topics and questions for the NATO STO Technical Activity Proposal (TAP) on AI-Augmented Wargaming for Capability Development. It incorporates all original questions from the NATO STO Potential Topics document plus recommended additions to address identified gaps.

**Central Research Question:**
> How can AI be responsibly integrated into wargaming to produce analytically credible, reusable, and decision-relevant insights that measurably improve NATO capability development and acquisition?

**Structure:** 10 research topics with 80 total research questions (36 original + 44 new)

---

## Topic 1: Methodological Foundations of AI-Augmented Wargaming

### Why This Matters
Leadership approval will hinge on whether this activity produces a repeatable, defensible methodology—not just isolated experiments. Without methodological rigor, AI-augmented wargaming risks being dismissed as technically impressive but analytically unreliable.

### Core Research Questions

**Original Questions:**

| ID | Research Question | Focus Area |
|----|-------------------|------------|
| RQ 1.1 | What elements of the wargaming lifecycle (problem framing, design, execution, analysis, exploitation) are most amenable to AI augmentation? | Scope definition |
| RQ 1.2 | How does AI-augmented wargaming differ methodologically from traditional human-only wargaming, simulation-based analysis, and OR studies? | Differentiation |
| RQ 1.3 | What minimum methodological standards are required for AI-supported wargames to be considered analytically credible for capability development? | Standards |
| RQ 1.4 | How can uncertainty, assumptions, and limitations introduced by AI be explicitly documented and audited? | Transparency |

**New Questions (Addressing Identified Gaps):**

| ID | Research Question | Gap Addressed |
|----|-------------------|---------------|
| RQ 1.5 | What empirical validation approaches can establish that AI-augmented wargaming produces demonstrably superior insights compared to traditional methods, and how should baseline performance be established? | Validation criteria |
| RQ 1.6 | Under what conditions (wargame type, participant expertise, problem domain) does the methodology produce reproducible results across different implementation teams? | Reproducibility |
| RQ 1.7 | What operational criteria define "analytical credibility" for AI-augmented wargaming, and who has authority to certify compliance with these criteria? | Credibility definition |

### Expected Advancement
- From ad hoc practice → standardized, NATO-relevant methodological framework
- Knowledge Readiness Level: KRL 2 → KRL 4

### Cross-STC Relevance
- **SAS:** Primary owner - analytical methodology
- **HFM:** Human factors in methodology design
- **IST:** Information systems for methodology implementation

---

## Topic 2: Human-AI Teaming and Cognitive Roles in Wargaming

### Why This Matters
Human factors and trust will determine adoption more than technical performance. If participants don't trust AI contributions or if AI degrades rather than enhances human judgment, the technology will fail regardless of its technical capabilities.

### Core Research Questions

**Original Questions:**

| ID | Research Question | Focus Area |
|----|-------------------|------------|
| RQ 2.1 | What cognitive roles should AI play in wargaming (e.g., facilitator, challenger, synthesizer, red-team assistant, recorder, analyst)? | Role definition |
| RQ 2.2 | How does AI influence group dynamics, cognitive bias, anchoring, and groupthink during wargame preparation and execution? | Bias effects |
| RQ 2.3 | Under what conditions does AI improve vs. degrade human judgment and creativity? | Performance conditions |
| RQ 2.4 | What training, guardrails, and interfaces are required to ensure humans remain accountable decision-makers? | Accountability |

**New Questions (Addressing Identified Gaps):**

| ID | Research Question | Gap Addressed |
|----|-------------------|---------------|
| RQ 2.5 | How should appropriate trust calibration be developed, and what indicators help humans know when to defer to vs. override AI recommendations? | Trust calibration |
| RQ 2.6 | What are the long-term effects of AI augmentation on human analyst expertise development and institutional analytic capacity? | Skill atrophy |
| RQ 2.7 | How do national and cultural differences in AI acceptance affect human-AI teaming effectiveness in multinational wargaming contexts? | Cultural variance |
| RQ 2.8 | What protocols should govern resolution when AI recommendations conflict with expert human judgment? | Disagreement resolution |

### Expected Advancement
- Empirical understanding of AI as a cognitive teammate rather than a tool
- TRL alignment: 3-5

### Cross-STC Relevance
- **HFM:** Primary owner - human factors
- **SAS:** Cognitive bias and decision-making
- **IST:** Interface design

---

## Topic 3: AI-Supported Generation of Core Wargaming Artifacts

### Why This Matters
This is where measurable productivity gains and quality improvements can be demonstrated. Artifact generation is also the most visible AI contribution, making it critical for stakeholder confidence.

### Core Research Questions

**Original Questions:**

| ID | Research Question | Focus Area |
|----|-------------------|------------|
| RQ 3.1 | Which artifacts can AI reliably generate or co-generate (e.g., ToR, scenarios, vignettes, ORBATs, mission threads, capability gap hypotheses)? | Artifact scope |
| RQ 3.2 | How can AI outputs be constrained to doctrinally, technically, and politically plausible bounds? | Constraint enforcement |
| RQ 3.3 | How should provenance, assumptions, and confidence levels of AI-generated artifacts be represented? | Provenance tracking |
| RQ 3.4 | How do AI-generated artifacts compare in quality, diversity, and analytical richness to human-generated ones? | Quality comparison |

**New Questions (Addressing Identified Gaps):**

| ID | Research Question | Gap Addressed |
|----|-------------------|---------------|
| RQ 3.5 | How can AI-generated artifacts be verified for correctness rather than merely plausibility, and what independent validation is required before operational use? | Verification |
| RQ 3.6 | What safeguards prevent sophisticated participants from gaming AI-generated scenarios by exploiting knowledge of AI behavior patterns? | Adversarial robustness |
| RQ 3.7 | How should AI systems handle scenario generation requests that fall outside their training distribution, and how are such boundary conditions detected? | Distribution shift |
| RQ 3.8 | What technical standards ensure AI-generated artifacts are interoperable with existing NATO data formats and systems (STANAGs, APP-6, etc.)? | Interoperability |

### Expected Advancement
- Proof-of-concept artifact pipelines suitable for NATO wargaming contexts
- TRL alignment: 4-6

### Cross-STC Relevance
- **SAS:** Scenario and analytical artifact design
- **MSG:** Simulation artifact integration
- **IST:** Data standards and interoperability

---

## Topic 4: Knowledge Management, Traceability, and Analytic Memory

### Why This Matters
NATO leadership is explicitly frustrated by insights that do not survive beyond a single event. Creating institutional analytic memory is essential for cumulative learning and evidence-based capability development.

### Core Research Questions

**Original Questions:**

| ID | Research Question | Focus Area |
|----|-------------------|------------|
| RQ 4.1 | How can wargaming knowledge (assumptions, decisions, arguments, evidence, outcomes) be structured for long-term reuse? | Knowledge structure |
| RQ 4.2 | What data and knowledge models best support traceability from scenario → gameplay → insight → recommendation? | Traceability |
| RQ 4.3 | How can AI support cross-game synthesis and longitudinal analysis of capability gaps? | Synthesis |
| RQ 4.4 | How should sensitive, classified, and releasable knowledge be partitioned and governed? | Classification |

**New Questions (Addressing Identified Gaps):**

| ID | Research Question | Gap Addressed |
|----|-------------------|---------------|
| RQ 4.5 | How should contradictions between insights from different wargames be detected, analyzed, and resolved to maintain knowledge base coherence? | Contradiction handling |
| RQ 4.6 | What criteria determine when wargaming knowledge has become obsolete, and what processes govern knowledge retirement? | Knowledge decay |
| RQ 4.7 | How should knowledge attribution and national caveats be managed when insights derive from multinational classified contributions? | Attribution |
| RQ 4.8 | How does the proposed knowledge management approach integrate with existing NATO and national knowledge management systems? | Systems integration |

### Expected Advancement
- Transition from episodic insight → institutional analytic memory
- KRL alignment: 3-5

### Cross-STC Relevance
- **IST:** Primary owner - information management
- **SAS:** Analytical knowledge structures
- **SCI:** Classification and security

---

## Topic 5: Integration with Modelling & Simulation (M&S) Ecosystems

### Why This Matters
MSG and SAS stakeholders will expect clarity on how AI-augmented wargaming complements—not replaces—existing M&S investments. Successful integration multiplies value; poor integration creates confusion and redundancy.

### Core Research Questions

**Original Questions:**

| ID | Research Question | Focus Area |
|----|-------------------|------------|
| RQ 5.1 | How can AI-supported qualitative wargaming be meaningfully coupled with quantitative M&S tools? | Coupling approach |
| RQ 5.2 | What interfaces, abstractions, or translation layers are required between narrative gameplay and simulation inputs/outputs? | Interface design |
| RQ 5.3 | How can AI assist in selecting, configuring, or interpreting M&S models during wargame design and analysis? | AI assistance |
| RQ 5.4 | Where are the limits of integration due to fidelity, data availability, or epistemic mismatch? | Integration limits |

**New Questions (Addressing Identified Gaps):**

| ID | Research Question | Gap Addressed |
|----|-------------------|---------------|
| RQ 5.5 | How should uncertainty from M&S models be propagated through and combined with uncertainty from human wargame judgments to produce calibrated confidence estimates? | Uncertainty propagation |
| RQ 5.6 | What evidence demonstrates that integrated qualitative-quantitative approaches produce superior insights compared to either method alone? | Integration validation |
| RQ 5.7 | How does the proposed integration architecture comply with existing M&S interoperability standards (HLA, DIS, SISO)? | Standards compliance |
| RQ 5.8 | What computational resources and latency constraints apply to real-time M&S integration during wargame execution? | Computational feasibility |

### Expected Advancement
- Practical integration patterns rather than theoretical alignment
- TRL alignment: 4-6

### Cross-STC Relevance
- **MSG:** Primary owner - modelling and simulation
- **SAS:** Analytical integration
- **IST:** Technical architecture

---

## Topic 6: Capability Gap Identification and Hypothesis Testing

### Why This Matters
This is the direct bridge to acquisition and DOTMLPFI decisions. The credibility of the entire research program depends on demonstrating that AI-augmented wargaming produces better capability gap identification than current methods.

### Core Research Questions

**Original Questions:**

| ID | Research Question | Focus Area |
|----|-------------------|------------|
| RQ 6.1 | How can AI help formulate, challenge, and refine capability gap hypotheses during wargaming? | Hypothesis formulation |
| RQ 6.2 | What constitutes sufficient evidentiary support for a gap identified through AI-augmented wargaming? | Evidence standards |
| RQ 6.3 | How can alternative explanations and confounding factors be systematically explored? | Alternative analysis |
| RQ 6.4 | How should confidence, risk, and prioritization be communicated to acquisition authorities? | Communication |

**New Questions (Addressing Identified Gaps):**

| ID | Research Question | Gap Addressed |
|----|-------------------|---------------|
| RQ 6.5 | How can false positive gap identification (spurious gaps from AI bias or wargame design artifacts) be distinguished from genuine capability shortfalls, and what evidentiary thresholds govern gap validation? | False discovery |
| RQ 6.6 | How should AI-identified capability gaps be mapped to DOTMLPFI solution spaces and integrated with existing NATO Defence Planning Process (NDPP) outputs? | DOTMLPFI mapping |
| RQ 6.7 | What framework governs gap prioritization when multiple gaps compete for limited acquisition resources? | Prioritization |
| RQ 6.8 | How should the temporal evolution of capability gaps be tracked and incorporated into dynamic planning processes? | Temporal dynamics |
| RQ 6.9 | What constitutes sufficient evidence to conclude that a hypothesized capability gap does NOT exist (negative findings)? | Negative evidence |

### Expected Advancement
- More rigorous, transparent gap identification processes
- TRL alignment: 5-6

### Cross-STC Relevance
- **SAS:** Primary owner - capability analysis
- **HFM:** Human performance gaps
- **AVT:** Technology gaps
- **SET:** System effectiveness gaps

---

## Topic 7: Sponsor Engagement, Transparency, and Decision Confidence

### Why This Matters
Senior sponsors must trust not only the outputs but the process. Without sponsor confidence, even excellent analysis will not influence decisions.

### Core Research Questions

**Original Questions:**

| ID | Research Question | Focus Area |
|----|-------------------|------------|
| RQ 7.1 | How can AI improve sponsor understanding of assumptions, trade-offs, and decision logic? | Understanding |
| RQ 7.2 | What visualization and narrative techniques best convey AI-supported insights to senior leaders? | Communication |
| RQ 7.3 | How does AI affect sponsor confidence, perceived legitimacy, and willingness to act? | Confidence |
| RQ 7.4 | What governance mechanisms are needed to ensure responsible use of AI in sponsor-facing contexts? | Governance |

**New Questions (Addressing Identified Gaps):**

| ID | Research Question | Gap Addressed |
|----|-------------------|---------------|
| RQ 7.5 | How should accountability be assigned when acquisition decisions informed by AI-augmented wargaming produce suboptimal outcomes? | Accountability |
| RQ 7.6 | What protocols govern situations where different sponsors or nations draw conflicting conclusions from the same AI-supported analysis? | Multi-stakeholder conflict |
| RQ 7.7 | How should AI limitations be communicated to sponsors in ways that calibrate expectations without undermining confidence in valid outputs? | Expectation management |
| RQ 7.8 | What escalation pathways should exist when AI-augmented analysis identifies findings with significant political or strategic implications? | Escalation |

### Expected Advancement
- Improved decision confidence and uptake of wargame results
- KRL alignment: 3-5

### Cross-STC Relevance
- **SAS:** Analytical communication
- **HFM:** Decision-maker cognition
- **SCI:** Security and political sensitivity

---

## Topic 8: Ethics, Governance, and Responsible Use of AI in Wargaming

### Why This Matters
This is essential for public releasability, multinational participation, and long-term legitimacy. NATO's adoption of AI wargaming must be consistent with Alliance values and legal frameworks.

### Core Research Questions

**Original Questions:**

| ID | Research Question | Focus Area |
|----|-------------------|------------|
| RQ 8.1 | What ethical risks arise from AI-supported scenario generation and adversary modeling? | Ethical risks |
| RQ 8.2 | How should bias, hallucination, and over-confidence be detected and mitigated? | Bias mitigation |
| RQ 8.3 | What governance models are appropriate for multinational AI-enabled analytic processes? | Governance models |
| RQ 8.4 | How do NATO values and legal frameworks constrain AI use in wargaming? | Legal constraints |

**New Questions (Addressing Identified Gaps):**

| ID | Research Question | Gap Addressed |
|----|-------------------|---------------|
| RQ 8.5 | What red lines should absolutely constrain AI behavior in wargaming (e.g., modeling specific real-world leaders, generating certain scenario types), and how are these enforced? | Red lines |
| RQ 8.6 | What governance applies to data used to train AI systems for wargaming, including consent, provenance, and retention requirements? | Data provenance |
| RQ 8.7 | How should emergent behaviors from multi-component AI systems be monitored, and what intervention mechanisms exist when unexpected behaviors occur? | Emergent behavior |
| RQ 8.8 | What audit trails and logging requirements ensure AI-influenced decisions can be reconstructed and reviewed post-hoc? | Audit mechanisms |
| RQ 8.9 | What safeguards prevent AI wargaming capabilities from being misused for unauthorized purposes or exploited by adversaries? | Dual-use risk |

### Expected Advancement
- Responsible AI guidance specific to wargaming and defence planning
- KRL alignment: 2-4

### Cross-STC Relevance
- **HFM:** Ethical decision-making
- **SAS:** Analytical governance
- **SCI:** Security implications
- **IST:** Technical safeguards

---

## Topic 9: Pathways to Operationalization and Future CDTs

### Why This Matters
STB and STCs will expect a credible path beyond study and reports. Research that cannot transition to operational use has limited value.

### Core Research Questions

**Original Questions:**

| ID | Research Question | Focus Area |
|----|-------------------|------------|
| RQ 9.1 | What elements are ready for transition into Cooperative Demonstrations of Technology (CDTs)? | CDT readiness |
| RQ 9.2 | What operational environments (education, exercises, acquisition support) are best suited for early adoption? | Adoption environments |
| RQ 9.3 | What technical and organizational barriers must be resolved before wider deployment? | Barriers |
| RQ 9.4 | How should success be measured in future trials? | Success metrics |

**New Questions (Addressing Identified Gaps):**

| ID | Research Question | Gap Addressed |
|----|-------------------|---------------|
| RQ 9.5 | What sustainment model (personnel, funding, governance, technology refresh) is required for long-term viability of AI-augmented wargaming capabilities across the Alliance? | Sustainment |
| RQ 9.6 | What workforce development and training pipeline is required to develop and maintain AI wargaming practitioners? | Workforce development |
| RQ 9.7 | What acquisition strategy (build, buy, partner, hybrid) best serves Alliance interests for AI wargaming capabilities? | Acquisition strategy |
| RQ 9.8 | What regression testing and continuous validation processes ensure capability does not degrade as AI models are updated or environments change? | Regression testing |
| RQ 9.9 | What cost-benefit framework should govern investment decisions in AI-augmented wargaming capabilities? | Cost-benefit analysis |

### Expected Advancement
- Clear roadmap from study → demonstration → adoption
- TRL alignment: 6-7 (for selected components)

### Cross-STC Relevance
- **All STCs:** Transition pathways
- **SAS:** Analytical capability deployment
- **MSG:** Simulation integration deployment

---

## Topic 10: Validation, Verification, and Comparative Effectiveness (NEW)

### Why This Matters
Without rigorous comparative evaluation, the entire research program risks producing capabilities that *feel* better but aren't demonstrably superior. This topic directly supports the overarching research question about "measurably improving" capability development and acquisition.

### Core Research Questions

| ID | Research Question | Focus Area |
|----|-------------------|------------|
| RQ 10.1 | How should baseline wargaming effectiveness be measured to enable rigorous comparison with AI-augmented approaches? | Baseline establishment |
| RQ 10.2 | What experimental designs can isolate the contribution of AI from other factors (facilitator skill, scenario quality, participant expertise, game mechanics)? | Experimental design |
| RQ 10.3 | How should AI-augmented wargaming be validated against historical cases, expert consensus, or operational outcomes? | Historical validation |
| RQ 10.4 | What constitutes sufficient evidence that AI augmentation improves the quality of acquisition decisions derived from wargaming? | Evidence standards |
| RQ 10.5 | How should comparative studies be designed to account for the uniqueness of individual wargames and the difficulty of controlled experimentation? | Study design |
| RQ 10.6 | What metrics beyond traditional wargaming evaluation (insight quality, decision confidence) should be used to assess AI contribution? | Novel metrics |

### Expected Advancement
- Rigorous evidence base for AI-augmented wargaming value proposition
- Empirically validated claims for sponsor engagement
- Foundation for defensible acquisition justification
- KRL alignment: 4-6 (moving from hypothesis to replication)

### Cross-STC Relevance
- **SAS:** Primary owner - analytical validation
- **HFM:** Human performance measurement
- **MSG:** Simulation validation methods

---

## Cross-Cutting Research Themes

The following themes span multiple topics and should be addressed through integrated research efforts:

### Theme A: Multinational Interoperability

**Relevant to:** Topics 2, 4, 7, 8

**Key Questions:**
- How do different national AI policies affect Alliance-wide adoption?
- What data sharing and classification frameworks enable multinational AI wargaming?
- How are national caveats managed in shared knowledge bases?

### Theme B: Failure Modes and Resilience

**Relevant to:** Topics 1, 2, 3, 6

**Key Questions:**
- What happens when AI fails, hallucinates, or produces biased outputs?
- How are failures detected in real-time during wargame execution?
- What fallback procedures maintain wargame integrity when AI components fail?

### Theme C: Adversary Considerations

**Relevant to:** Topics 3, 6, 8

**Key Questions:**
- How might adversaries exploit knowledge of NATO AI wargaming methods?
- Can AI-generated scenarios be manipulated through adversarial inputs?
- What security measures protect AI wargaming capabilities?

### Theme D: Technical Architecture

**Relevant to:** Topics 3, 4, 5, 9

**Key Questions:**
- What infrastructure (cloud, on-premise, hybrid) supports secure AI wargaming?
- How is data sovereignty maintained across nations?
- What computational resources are required for operational deployment?

### Theme E: Mission Engineering Integration

**Relevant to:** Topics 3, 4, 6, 7

**Key Questions:**
- How do AI-identified gaps map to Mission Engineering artifacts?
- What interfaces connect wargaming insights to acquisition milestones?
- How does AI-augmented wargaming integrate with JCIDS and NDPP processes?

---

## Summary Statistics

| Category | Count |
|----------|-------|
| **Total Topics** | 10 |
| **Original Questions** | 36 |
| **New Questions** | 44 |
| **Total Questions** | 80 |
| **Cross-Cutting Themes** | 5 |

### Questions by Topic

| Topic | Original | New | Total |
|-------|----------|-----|-------|
| 1. Methodology | 4 | 3 | 7 |
| 2. Human-AI Teaming | 4 | 4 | 8 |
| 3. Artifact Generation | 4 | 4 | 8 |
| 4. Knowledge Management | 4 | 4 | 8 |
| 5. M&S Integration | 4 | 4 | 8 |
| 6. Gap Identification | 4 | 5 | 9 |
| 7. Sponsor Engagement | 4 | 4 | 8 |
| 8. Ethics/Governance | 4 | 5 | 9 |
| 9. Operationalization | 4 | 5 | 9 |
| 10. Validation (NEW) | 0 | 6 | 6 |
| **Total** | **36** | **44** | **80** |

---

## Readiness Level Targets

| Topic | TRL Target | KRL Target | Primary Advancement |
|-------|------------|------------|---------------------|
| 1. Methodology | - | 2→4 | Standardized framework |
| 2. Human-AI Teaming | 3-5 | - | Empirical teaming understanding |
| 3. Artifact Generation | 4-6 | - | PoC artifact pipelines |
| 4. Knowledge Management | - | 3-5 | Institutional memory |
| 5. M&S Integration | 4-6 | - | Practical integration patterns |
| 6. Gap Identification | 5-6 | - | Rigorous gap processes |
| 7. Sponsor Engagement | - | 3-5 | Decision confidence |
| 8. Ethics/Governance | - | 2-4 | Responsible AI guidance |
| 9. Operationalization | 6-7 | - | Transition roadmap |
| 10. Validation | - | 4-6 | Evidence base |

---

## STC Relevance Matrix

| Topic | SAS | MSG | HFM | IST | SCI | AVT | SET |
|-------|-----|-----|-----|-----|-----|-----|-----|
| 1. Methodology | **P** | - | S | S | - | - | - |
| 2. Human-AI Teaming | S | - | **P** | S | - | - | - |
| 3. Artifact Generation | **P** | S | - | S | - | - | - |
| 4. Knowledge Management | S | - | - | **P** | S | - | - |
| 5. M&S Integration | S | **P** | - | S | - | - | - |
| 6. Gap Identification | **P** | - | S | - | - | S | S |
| 7. Sponsor Engagement | **P** | - | S | - | S | - | - |
| 8. Ethics/Governance | S | - | **P** | S | S | - | - |
| 9. Operationalization | S | S | - | S | - | - | - |
| 10. Validation | **P** | S | S | - | - | - | - |

**Legend:** P = Primary, S = Secondary

---

## Next Steps for TAP Development

1. **Prioritize questions** based on feasibility and sponsor interest
2. **Map questions to specific deliverables** and work packages
3. **Identify national contributions** aligned with question ownership
4. **Develop experimental wargame case studies** that address multiple questions
5. **Draft evaluation criteria** for each research question
6. **Estimate resource requirements** by topic and question

---

*Document prepared for NATO STO TAP development.*
*Version 1.0 - 2026-01-22*
