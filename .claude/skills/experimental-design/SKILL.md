---
name: Experimental Design
version: 1.0.0
description: Methodology for testing ideas systematically through hypothesis formulation, test design, and result interpretation
category: Philosophical Research Methods
difficulty: Advanced
estimatedTime: Variable (depends on experiment complexity)
---

# Experimental Design

A systematic methodology for testing ideas through well-designed experiments. Based on pragmatist experimental philosophy and scientific method, this skill helps researchers formulate testable hypotheses, design rigorous tests, and interpret results appropriately.

## What This Skill Does

Experimental Design enables researchers to:
- Formulate clear, testable hypotheses
- Design experiments that genuinely test hypotheses
- Execute tests and collect valid data
- Interpret results and draw warranted conclusions

## Prerequisites

- Basic understanding of scientific method
- Familiarity with hypothesis testing concepts
- Some knowledge of research design principles

---

## Core Concepts

### The Experimental Mindset

```
PRAGMATIST EXPERIMENTAL APPROACH

Ideas are tested by their consequences:
- What difference does this idea make?
- What would we observe if true?
- What would we observe if false?
- How can we check?

All beliefs are hypotheses:
- Subject to testing
- Revisable based on evidence
- Held tentatively until tested

Inquiry is experimental:
- We learn by trying
- Results inform revision
- Knowledge grows through testing
```

### Elements of an Experiment

```
HYPOTHESIS (H)
What we're testing
- Clear statement
- Testable prediction
- Falsifiable

CONDITIONS (C)
Context of the test
- What we control
- What we measure
- What we hold constant

PROCEDURE (P)
How we test
- Step-by-step protocol
- Data collection method
- Analysis plan

CRITERIA (K)
What counts as support/failure
- Success criteria
- Failure criteria
- Ambiguous outcomes
```

---

## Hypothesis Formulation

### Characteristics of Good Hypotheses

| Characteristic | Description | Test |
|----------------|-------------|------|
| **Clear** | Unambiguous statement | Can others understand exactly what's claimed? |
| **Specific** | Precise prediction | What exactly would we observe? |
| **Testable** | Can be checked | Can we design a test? |
| **Falsifiable** | Can be proven wrong | What would disconfirm it? |
| **Relevant** | Matters for the question | Does answering this help? |

### Hypothesis Formulation Protocol

```
Phase 1: State the Question
- What are we trying to learn?
- What is uncertain?

Phase 2: Generate Candidate Hypotheses
- What are possible answers?
- What explanations are plausible?

Phase 3: Clarify Each Hypothesis
- What exactly does it claim?
- Define key terms
- Specify scope

Phase 4: Derive Predictions
- If H is true, what should we observe?
- If H is false, what should we observe?
- What would distinguish H from alternatives?

Phase 5: Assess Testability
- Can we actually test this?
- What would be needed?
- Is it feasible?
```

### Hypothesis Formulation Template

```
HYPOTHESIS: [Clear statement]

DEFINITIONS:
- [Key term 1]: [Definition]
- [Key term 2]: [Definition]

IF TRUE:
- We would observe: [Prediction 1]
- We would observe: [Prediction 2]

IF FALSE:
- We would observe: [Prediction 3]
- We would observe: [Prediction 4]

DISTINGUISHING TEST:
- To distinguish from [alternative], look for [observation]
```

---

## Test Design

### Design Principles

```
INTERNAL VALIDITY
Does the test actually test the hypothesis?
- Manipulation affects outcome
- No confounding factors
- Alternative explanations ruled out

EXTERNAL VALIDITY
Do results generalize?
- Sample represents population
- Conditions represent real world
- Context not too artificial

RELIABILITY
Would we get the same result again?
- Consistent measurement
- Replicable procedure
- Stable outcome

SENSITIVITY
Can the test detect a real effect?
- Enough power
- Adequate sample
- Appropriate measures
```

### Design Process

```
┌────────────────────────────────────────────────────────┐
│                                                        │
│  1. IDENTIFY WHAT TO TEST                              │
│     └─> What prediction will we check?                 │
│         What comparison will we make?                  │
│                                                        │
│  2. OPERATIONALIZE CONCEPTS                            │
│     └─> How will we measure outcomes?                  │
│         How will we create conditions?                 │
│                                                        │
│  3. CONTROL CONFOUNDS                                  │
│     └─> What else could affect outcome?                │
│         How will we rule out alternatives?             │
│                                                        │
│  4. SPECIFY PROCEDURE                                  │
│     └─> What steps exactly?                            │
│         What sequence?                                 │
│                                                        │
│  5. DEFINE SUCCESS CRITERIA                            │
│     └─> What outcome supports H?                       │
│         What outcome refutes H?                        │
│         What's ambiguous?                              │
│                                                        │
│  6. PLAN ANALYSIS                                      │
│     └─> How will results be analyzed?                  │
│         What comparisons will be made?                 │
│                                                        │
└────────────────────────────────────────────────────────┘
```

### Test Design Template

```
TEST DESIGN: [Name]

HYPOTHESIS BEING TESTED:
[Statement]

PREDICTION:
[What we expect if H is true]

OPERATIONALIZATIONS:
- Independent variable: [What we manipulate/compare]
- Dependent variable: [What we measure]
- Operational definitions: [How concepts are measured]

CONDITIONS:
- Test condition: [Description]
- Control/comparison: [Description]
- Controls for: [Confounds addressed]

PROCEDURE:
1. [Step 1]
2. [Step 2]
3. [Step 3]
...

SUCCESS CRITERIA:
- H supported if: [Outcome]
- H refuted if: [Outcome]
- Ambiguous if: [Outcome]

ANALYSIS PLAN:
[How results will be analyzed]
```

---

## Types of Tests

### Test Type Selection

| Type | Use When | Example |
|------|----------|---------|
| **Crucial Test** | Two hypotheses make different predictions | Does A or B better predict outcome? |
| **Confirmation Test** | Seeking evidence for H | Does predicted pattern appear? |
| **Disconfirmation Test** | Challenging H rigorously | Can we find counter-evidence? |
| **Parameter Test** | Quantifying relationship | How much effect does X have? |
| **Exploratory Test** | Gathering initial data | What happens when we try X? |
| **Replication Test** | Verifying previous result | Does finding hold again? |

### Test Strength Hierarchy

```
STRONGEST TO WEAKEST:

1. CRUCIAL TEST
   - Distinguishes between competing hypotheses
   - Predictions conflict; one must fail
   - Maximum information value

2. RISKY PREDICTION
   - Hypothesis makes specific, unlikely prediction
   - Confirmation very informative
   - Failure would be surprising

3. NOVEL PREDICTION
   - Hypothesis predicts something not yet observed
   - Prediction made before data gathered
   - Tests predictive power

4. ACCOMMODATION
   - Hypothesis explains known data
   - Post-hoc fit
   - Less informative than prediction

5. AD HOC ADJUSTMENT
   - Hypothesis modified to fit
   - No independent test
   - Weak support
```

---

## Result Interpretation

### Interpretation Framework

```
RESULT SCENARIOS

STRONG CONFIRMATION
- Clear positive result
- Prediction matched closely
- No obvious alternative explanation
→ Substantial support for H

MODERATE SUPPORT
- Positive but noisy result
- Prediction approximately matched
- Some alternative explanations possible
→ Tentative support, test further

INCONCLUSIVE
- Results unclear
- Doesn't clearly distinguish
- Multiple interpretations possible
→ Design better test

MODERATE DISCONFIRMATION
- Prediction not matched
- But methodology questions exist
- Or scope might be limited
→ Question H, but consider limitations

STRONG DISCONFIRMATION
- Clear negative result
- Prediction clearly failed
- No obvious methodological escape
→ Revise or reject H
```

### Interpretation Checklist

```
Before concluding:

□ Did test actually test H?
□ Were conditions as intended?
□ Were measures valid?
□ Are alternative explanations ruled out?
□ Is sample adequate?
□ Would result replicate?
□ What confidence is warranted?
□ What remains uncertain?
```

### Common Interpretation Errors

| Error | Description | Avoidance |
|-------|-------------|-----------|
| Over-interpretation | Reading too much into results | State conclusions cautiously |
| Under-interpretation | Missing what results show | Consider all implications |
| Confirmation bias | Seeing only supporting evidence | Actively seek disconfirmation |
| Single-study overconfidence | Too much from one test | Require replication |
| Null result misinterpretation | "No result = H false" | May just be weak test |

---

## Integration with Claude Flow

### Spawning Experimental Analysis

```bash
# Run experimental design
claude-flow hive-mind spawn "Test [hypothesis]" \
  --queen pragmatic \
  --workers experimentalist,hypothesis-generator,consequence-tracer
```

### Memory Patterns

```javascript
// Store experimental design and results
mcp__claude-flow__memory_usage({
  action: "store",
  key: "experiment/design/[name]",
  namespace: "philosophical",
  value: JSON.stringify({
    hypothesis: hypothesisStatement,
    predictions: derivedPredictions,
    design: testDesign,
    procedure: stepByStepProcedure,
    criteria: successFailureCriteria,
    results: observedOutcomes,
    interpretation: warrantsConclusions,
    nextSteps: followUpPlanning
  })
})
```

---

## Output Templates

### Experiment Design Document

```
EXPERIMENT: [Name]
Date: [Date]
Version: [N]

BACKGROUND
Research question: [Question]
Why it matters: [Importance]

HYPOTHESIS
Statement: [H]
Source: [How generated]

PREDICTIONS
If H true: [Predictions]
If H false: [Counter-predictions]

DESIGN
Type: [Crucial/Confirmation/etc.]
Independent variable: [Description]
Dependent variable: [Description]
Controls: [List]

PROCEDURE
Materials: [List]
Steps:
1. [Step]
2. [Step]
...

SUCCESS CRITERIA
Support H if: [Criterion]
Refute H if: [Criterion]
Ambiguous if: [Criterion]

ANALYSIS PLAN
Method: [How analyzed]
Comparisons: [What compared]
```

### Experiment Results Report

```
RESULTS REPORT: [Experiment Name]
Date: [Date]

HYPOTHESIS TESTED
[Statement]

DESIGN SUMMARY
[Brief description]

RESULTS
Observations: [Data]
Analysis: [Statistical/qualitative]
Outcome: [Support/Refute/Ambiguous]

INTERPRETATION
What this means: [Analysis]
Confidence level: [Assessment]
Limitations: [Caveats]

CONCLUSIONS
H status: [Supported/Refuted/Uncertain]
Warranted belief: [What we now know]

NEXT STEPS
If follow-up needed: [Plans]
If replication needed: [Plans]
New questions: [List]
```

---

## Quality Criteria

Experimental design is successful when:

1. **Testable**: Hypothesis can actually be tested
2. **Valid**: Test genuinely tests H
3. **Controlled**: Confounds are addressed
4. **Clear**: Criteria defined in advance
5. **Documented**: Process fully recorded
6. **Honest**: Results reported accurately

---

## Troubleshooting

### "Hypothesis isn't testable"
→ Make more specific
→ Derive observable predictions
→ Operationalize concepts differently

### "Can't control confounds"
→ Accept limitations, note them
→ Use comparison conditions
→ Multiple tests from different angles

### "Results are ambiguous"
→ Refine criteria
→ Increase sample/precision
→ Design more discriminating test

### "Negative result but H might be true"
→ Check if test was sensitive enough
→ Consider scope limitations
→ Design alternative test

---

## Learn More

- Fisher, R.A. (1935). The Design of Experiments
- Popper, K. (1959). The Logic of Scientific Discovery
- Mayo, D. (1996). Error and the Growth of Experimental Knowledge
